---
title: "p8105_hw5_xz3173"
author: "Xue Zhang"
date: "2023-11-02"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(broom)
```

# Problem 1

```{r}
# Load homicide dataset
homicide_df = 
  read_csv("data/homicide-data.csv") |>
  janitor::clean_names()

# Explore data
str(homicide_df)
summary(homicide_df)
```

**Describe the raw data.** 

**The `homicide_df` dataset contains `r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns, with each row representing a single homicide case report from the data that the Washington Post has collected in 50 large U.S. cities. There are several victim-related variables, such as first name, last name, race, age, sex of victims. There are also homicide-related variables, such as the location of the killing, whether an arrest was made.**


```{r}
# Create a city_state variable
city_summary = homicide_df |>
  mutate(city_state = str_c(city, state, sep = ", ")) |>

# Summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”)
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
```

For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
# Filter data for Baltimore, MD
baltimore_df = city_summary |>
  filter(city_state == "Baltimore, MD") 

# Using prop.test to estimate the proportion of unsolved homicides
prop_test_result = 
  prop.test(
    x = pull(baltimore_df, unsolved_homicides),
    n = pull(baltimore_df, total_homicides)) 

# Tidying the result using broom:tidy
tidied_result = tidy(prop_test_result)

# Extracting the estimated proportion and confidence intervals
estimated_proportion = pull(tidied_result, estimate)
conf_low = pull(tidied_result, conf.low)
conf_high = pull(tidied_result, conf.high)
```

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r}
# Run prop.test for each city and create a tidy dataframe
results_df = city_summary |>
  mutate(
    prop_test_result = map2(
      unsolved_homicides,
      total_homicides,
      ~prop.test(x = .x, n = .y)
    )
  ) |>
  mutate(tidied_result = map(prop_test_result, broom::tidy)) |>
  unnest(tidied_result) |>
  select(city_state, estimate, conf.low, conf.high)
```

Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r}
# Create the plot
plot = 
  ggplot(
    results_df, 
    aes(
      x = reorder(city_state, -estimate), 
      y = estimate)) +
  geom_point(color = "blue", size = 3) +
  geom_errorbar(
    aes(
      ymin = conf.low,
      ymax = conf.high), 
    width = 0.2,
    color = "red") +
  coord_flip() +
  labs(
    title = "Proportiton of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides") +
  theme_minimal()

plot
```



# Problem 2

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time:

```{r}
# Start with a dataframe containing all file names; the list.files function will help
file_names = list.files(path = "data2", pattern = "*.csv", full.names = TRUE)
file_info = tibble(file = file_names)
```

```{r}
# Iterate over file names and read in data for each subject using purrr::map and saving the result as a new variable in the dataframe
file_info = file_info |>
  mutate(data = map(file, read_csv))
```

```{r}
# Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary
tidy_data = file_info |>
  mutate(
    subject_id = str_extract(file, "(?<=con_|exp_)[0-9]+"), 
    arm = ifelse(str_detect(file, "con"), "con", "exp")) |>
  select(subject_id, arm, data) |>
  unnest(cols = c(data)) |>
  pivot_longer(
    cols = starts_with("week"),
    names_to = "week",
    values_to = "observation") |>
  mutate(
    week = str_extract(week, "[0-9]+") |>
      as.numeric()) 
```

```{r}
# Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.
ggplot(
  tidy_data, 
  aes(
    x = week,
    y = observation, 
    group = subject_id,
    color = arm)) +
  geom_line(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Spaghetti Plot of Observations over Time",
       x = "Week",
       y = "Observation") 
```

**Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.**

**Upon examining the spaghetti plot, we observe distinct patterns between the control and experimental groups. Subjects in the control group (`con`) exhibit relatively stable observation values over the weeks, with minor fluctuations around a consistent mean. This suggests a lack of significant change or impact over time within the control group.**

**In contrast, subjects in the experimental group (`exp`) display a more pronounced trend. Starting from week 3, there's a noticeable upwward trajectory in the observation values, indicating a potential effect of the experimental condition being tested. This trend becomes more evident in the later weeks, suggesting a cumulative or time-dependent effect.**

**Furthermore, the variability within the experimental group appears higher compared to the control group. While most subjects in the experimental arm follow the general upward trend, a few outliers do not conform to this pattern, indicating individual differences in response to the experimental condition.**

**Overall, the spaghetti plot suggests that the experimental condition may have a significant impact on the observation variable, especially when compared to the stable patterns observed in the control group. However, the presence of outliers and variability within the experimental group warrants a deeper investigation into individual response mechanisms.**



# Problem 3

When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a one-sample t-test.

First set the following design elements:

Fix n=30
Fix σ=5
Set μ=0
. Generate 5000 datasets from the model

x∼Normal[μ,σ]

For each dataset, save μ̂ 
 and the p-value arising from a test of H:μ=0
 using α=0.05
. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.

Repeat the above for μ={1,2,3,4,5,6}
, and complete the following:




```{r}
# Define the simulation function
sim_t_test = function(n = 30, mu = 0, sigma =5) {
    x = rnorm(n, mean = mu, sd = sigma)
    test_result = t.test(x, mu = 0)
    tidy(test_result)
}
```

```{r}
mu_values = 0:6
alpha = 0.05

# Create a grid for simulations
sim_grid =
  expand_grid(
    mu = mu_values,
    iter = 1:5000
    ) 
```

```{r}
# Perform simulations
all_results = sim_grid |>
  mutate(
    sim_results = 
      map2(
        mu,
        iter,
        ~sim_t_test(
          n = 30,
          mu = .x, 
          sigma = 5))) |>
  select(-iter) |>
  unnest(sim_results) |>
  mutate(
    rejected_null = p.value < alpha,
    mu = as.numeric(mu)
  )
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. 
 
```{r}
# Calculate power for each mu
power_analysis = all_results |>
  group_by(mu) |>
  summarize(power = mean(rejected_null))

power_analysis
```

```{r}
ggplot(power_analysis, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  labs(
    title = "Power of the Test vs. True mean (mu)",
    x = "True Mean (mu)",
    y = "Power of the Test"
  )
```

**Describe the association between effect size and power.**

**In the generated plot, we observe a clear positive association between the effect size (true mean, mu) and the power of the test. As the value of mu increases, moving away from the null hypothesis of mu = 0, the power of the test also increases. This trend illustrates a fundamental principle of hypothesis testing: larger effect sizes generally make it easier for a test to detect significant differences from the null hypothesis.**

**At lower values of mu (closer to 0), the power of the test is relatively low. This indicates that when the true mean is close to the value under the null hypothesis, our test has a reduced ability to correctly reject the null hypothesis. In particular terms, this means there's a higher chance of committing a Type II error (failing to detect a true effect) when the effect size is small.**

**As mu increases, the power of the test rises sharply. This increase in power suggests that our test becomes more effective at detecting true effects as they become more pronounced. When the effect size is large (mu significantly greater than 0), the likelihood that our test will correctly reject the null hypothesis is higher. Thus, the test demonstrates greater sensitivity to larger effect sizes.**

**This relationship underscores the importance of considering effect size in experimental design and analysis. In scenarios where small effect sizes are expected, larger sample sizes might be necessary to achieve adequate power. Conversely, for large effect sizes, even a smaller sample might suffice to yield reliable results. The plot demonstrates that the ability of a statistical test to uncover true effects is heavily dependent on the magnitude of these effects relative to the null hypothesis.**


Make a plot showing the average estimate of μ̂ on the y axis and the true value of μ on the x axis. Make a second plot (or overlay on the first) the average estimate of μ̂ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. 


```{r}
# Calculate average estimates
average_estimates = all_results |>
  group_by(mu) |>
  summarize(
    overall_mean_mu_hat = mean(estimate),
    mean_mu_hat_when_rejected = mean(estimate[rejected_null], na.rm = TRUE)
  )

average_estimates
```

**Overall mean mu_hat: For each true mean value, from 0 to 6, the overall estimate of mu_hat is very close to the true mean. This indicates that, on average, the tests provide unbiased estimates of the true mean.**

**Mean mu when null rejected: When mu = 0, the mean estimate of mu_hat when the null is rejected is 0.220, which is notably higher than the true mean of 0. This is a clear indication of bias due to only considering samples where the null is rejected. As mu increases, the mean estimate of mu_hat in cases where the null is rejected gets closer to the true mean. For mu = 1, the estimate is 2.24, but by the time mu reaches 6, the estimate (5.99) is nearly identical to the true mean.**

```{r}
ggplot(average_estimates, aes(x = mu)) +
  geom_line(aes(y = overall_mean_mu_hat), color = "blue") +
  geom_line(aes(y = mean_mu_hat_when_rejected), color = "red") +
  geom_point(aes(y = overall_mean_mu_hat), color = "blue") +
  geom_point(aes(y = mean_mu_hat_when_rejected), color = "red") +
  theme_minimal() +
  labs(
    title = "Average Estimates of mu_hat vs. True Mean (mu)",
    x = "True Mean (mu)",
    y = "Average Estimate of mu_hat")
```

**Is the sample average of μ̂ across tests for which the null is rejected approximately equal to the true value of μ ? Why or why not?**

**For smaller values of mu (like 0 and 1), the sample average of mu_hat across tests for which the null is rejected is not approximately equal to the true value of mu. This discrepancy is due to the phenomenon of "winner's curse," where the estimates are biased upwards in cases where the null hypothesis is rejected. When the true effect size is small, samples that significantly deviate from the null hypothesis are more likely to be due to random chance, leading to an overestimation of the effect size.**

**For larger values of mu (4, 5, 6), the sample average of mu_hat across tests for which the null is rejected closely approximates the true value of mu. In these cases, the effect size is large enough that the sample estimates where the null is rejected are not just due to random variations but are genuinely reflecting the true effect size.**

**Conclusion: The data shows that the bias in estimates of mu_hat when considering only samples where the null is rejected depends on the true effect size. For small effect sizes, this bias is significant, leading to overestimation. For large effect sizes, the bias diminishes, and the estimates are much more accurate. This insight is crucial in interpreting results from statistical testts, especially in cases where significant results are preferentially reportted or considered.**
